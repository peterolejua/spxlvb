alpha_prior_precision_grid = alpha_prior_precision_grid,
b_prior_precision = rep(1, ncol(sim_data$X)), # Fixed slab precision for CV
standardize = TRUE,
intercept = TRUE,
max_iter = 100L,
tol = 1e-05,
seed = 12376,
verbose = TRUE,
parallel = TRUE
)
cv_fit$CVE
# Define search grids
alpha_prior_precision_grid <- c(0, 5, 10, 100, 1000)
b_prior_precision_grid <- c(0.1, 0.5,1,1.5)
opt_fit <- grid.search.spxlvb.fit(
X = sim_data$X,
Y = sim_data$Y,
X_validation = sim_data$X_validation,
Y_validation = sim_data$Y_validation,
beta_true = sim_data$beta,
mu_0 = initials$mu_0,
omega_0 = initials$omega_0,
c_pi_0 = initials$c_pi_0,
d_pi_0 = initials$d_pi_0,
tau_e = initials$tau_e,
update_order = initials$update_order,
mu_alpha = rep(1, ncol(sim_data$X) + 1),
alpha_prior_precision_grid = alpha_prior_precision_grid,
b_prior_precision_grid = b_prior_precision_grid,
standardize = TRUE,
intercept = TRUE,
max_iter = 100L,
tol = 1e-05,
seed = 12376,
verbose = TRUE,
parallel = TRUE
)
opt_fit$optimal_hyper
cat("Optimal Alpha Precision:", opt_fit$optimal_hyper[1], "\n")
plot(sim_data$beta)
plot(fit$beta)
plot(opt_fit$fit_spxlvb$beta)
plot(cv_fit$fit_spxlvb$beta)
calculate_metrics <- function(
beta_hat,
posterior_inclusion_probability,
sim_data,
alpha = 0.1
) {
# Predictions
eta_true_test <- sim_data$X_test %*% sim_data$beta
eta_hat_test <- cbind(1,sim_data$X_test) %*% beta_hat
# Estimation Error
mse_beta <- mean((sim_data$beta - beta_hat[-1])^2)
mse_eta  <- mean((eta_true_test - eta_hat_test)^2)
# Selection Performance (using local false discovery rate)
lfdr_val = 1 - posterior_inclusion_probability
lfdr_val[is.na(lfdr_val)] <- 1
threshold <- 0
if (min(lfdr_val) < alpha) {
threshold <- max(sort(lfdr_val)[cumsum(sort(lfdr_val)) < alpha])
}
# Then, you would select all predictors where
#posterior_inclusion_probability >= 1-threshold.
# Confusion matrix components
TP <- (posterior_inclusion_probability >= (1-threshold)) & (sim_data$beta != 0)
FP <- (posterior_inclusion_probability >= 1-threshold) & (sim_data$beta == 0)
FN <- (posterior_inclusion_probability < 1-threshold) & (sim_data$beta != 0)
TPR <- sum(TP) / max(sum(TP) + sum(FN), 1)
FDR <- sum(FP) / max(sum(TP) + sum(FP), 1)
return(
list(
mse_beta = mse_beta,
mse_eta = mse_eta,
TPR = TPR,
FDR = FDR)
)
}
metrics_fit <- calculate_metrics(
beta_hat = fit$beta,
posterior_inclusion_probability = fit$omega, # varies per method
sim_data = sim_data,
alpha = 0.1
)
metrics_cv_fit <- calculate_metrics(
beta_hat = cv_fit$fit_spxlvb$beta,
posterior_inclusion_probability = cv_fit$fit_spxlvb$omega, # varies per method
sim_data = sim_data,
alpha = 0.1
)
metrics_opt_fit  <- calculate_metrics(
beta_hat = opt_fit$fit_spxlvb$beta,
posterior_inclusion_probability = opt_fit$fit_spxlvb$omega, # varies per method
sim_data = sim_data,
alpha = 0.1
)
metrics_df <- rbind(
metrics_fit,
metrics_cv_fit,
metrics_opt_fit
)
metrics_df
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(spxlvb)
library(doParallel)
cl <- makeCluster(min(2, parallel::detectCores()))
registerDoParallel( cl)
set.seed(17)
n <- 200     # Sample size
p <- 1000    # Number of predictors
pi_0 <- 0.05 # Sparsity level (5% of predictors are active)
# Pre-calculate the Cholesky factor for a Matérn correlation matrix
L_mat <- get.L(p = p, range = 2.0, smoothness = 1.5)
# Generate the data
sim_data <- matern.data.gen(
seed_val = 42,
n = n,
p = p,
pi_0 = pi_0,
L = L_mat,
sd_beta = 4,   # Standard deviation of true signals
SNR = 2,        # Signal-to-Noise Ratio
n_test = n,
n_validation = n
)
cat("Number of active predictors:", sum(sim_data$beta != 0), "\n")
initials <- get.initials.spxlvb(
X = sim_data$X,
Y = sim_data$Y,
mu_0 = NULL,
omega_0 = NULL,
c_pi_0 = NULL,
d_pi_0 = NULL,
tau_e = NULL,
update_order = NULL,
seed = 12376
)
## Fit the Parameter Exploded Variational Bayes model
fit <- spxlvb(
X = sim_data$X,
Y = sim_data$Y,
mu_0 = initials$mu_0,
omega_0 = initials$omega_0,
c_pi_0 = initials$c_pi_0,
d_pi_0 = initials$d_pi_0,
tau_e = initials$tau_e,
update_order = initials$update_order,
mu_alpha = rep(1, ncol(sim_data$X) + 1),
alpha_prior_precision = 1000,
b_prior_precision = rep(1, ncol(sim_data$X)),
standardize = T,
intercept = T,
max_iter = 1000,
tol = 0.001,
seed = 12376
)
cat("Model converged in", fit$iterations, "iterations.\n")
# Define search grids
alpha_prior_precision_grid <- c(0, 5, 10, 100, 1000)
# Perform 5-fold cross-validation
cv_fit <- cv.spxlvb.fit(
k = 5,
X = sim_data$X,
Y = sim_data$Y,
mu_0 = initials$mu_0,
omega_0 = initials$omega_0,
c_pi_0 = initials$c_pi_0,
d_pi_0 = initials$d_pi_0,
tau_e = initials$tau_e,
update_order = initials$update_order,
mu_alpha = rep(1, ncol(sim_data$X) + 1),
alpha_prior_precision_grid = alpha_prior_precision_grid,
b_prior_precision = rep(1, ncol(sim_data$X)), # Fixed slab precision for CV
standardize = TRUE,
intercept = TRUE,
max_iter = 100L,
tol = 1e-05,
seed = 12376,
verbose = TRUE,
parallel = TRUE
)
cv_fit$CVE
# Define search grids
alpha_prior_precision_grid <- c(0, 5, 10, 100, 1000)
b_prior_precision_grid <- c(0.1, 0.5,1,1.5)
opt_fit <- grid.search.spxlvb.fit(
X = sim_data$X,
Y = sim_data$Y,
X_validation = sim_data$X_validation,
Y_validation = sim_data$Y_validation,
beta_true = sim_data$beta,
mu_0 = initials$mu_0,
omega_0 = initials$omega_0,
c_pi_0 = initials$c_pi_0,
d_pi_0 = initials$d_pi_0,
tau_e = initials$tau_e,
update_order = initials$update_order,
mu_alpha = rep(1, ncol(sim_data$X) + 1),
alpha_prior_precision_grid = alpha_prior_precision_grid,
b_prior_precision_grid = b_prior_precision_grid,
standardize = TRUE,
intercept = TRUE,
max_iter = 100L,
tol = 1e-05,
seed = 12376,
verbose = TRUE,
parallel = TRUE
)
opt_fit$optimal_hyper
cat("Optimal Alpha Precision:", opt_fit$optimal_hyper[1], "\n")
plot(sim_data$beta)
plot(fit$beta)
plot(opt_fit$fit_spxlvb$beta)
plot(cv_fit$fit_spxlvb$beta)
calculate_metrics <- function(
beta_hat,
posterior_inclusion_probability,
sim_data,
alpha = 0.1
) {
# Predictions
eta_true_test <- sim_data$X_test %*% sim_data$beta
eta_hat_test <- cbind(1,sim_data$X_test) %*% beta_hat
# Estimation Error
mse_beta <- mean((sim_data$beta - beta_hat[-1])^2)
mse_eta  <- mean((eta_true_test - eta_hat_test)^2)
# Selection Performance (using local false discovery rate)
lfdr_val = 1 - posterior_inclusion_probability
lfdr_val[is.na(lfdr_val)] <- 1
threshold <- 0
if (min(lfdr_val) < alpha) {
threshold <- max(sort(lfdr_val)[cumsum(sort(lfdr_val)) < alpha])
}
# Then, you would select all predictors where
#posterior_inclusion_probability >= 1-threshold.
# Confusion matrix components
TP <- (posterior_inclusion_probability >= (1-threshold)) & (sim_data$beta != 0)
FP <- (posterior_inclusion_probability >= 1-threshold) & (sim_data$beta == 0)
FN <- (posterior_inclusion_probability < 1-threshold) & (sim_data$beta != 0)
TPR <- sum(TP) / max(sum(TP) + sum(FN), 1)
FDR <- sum(FP) / max(sum(TP) + sum(FP), 1)
return(
list(
mse_beta = mse_beta,
mse_eta = mse_eta,
TPR = TPR,
FDR = FDR)
)
}
metrics_fit <- calculate_metrics(
beta_hat = fit$beta,
posterior_inclusion_probability = fit$omega, # varies per method
sim_data = sim_data,
alpha = 0.1
)
metrics_cv_fit <- calculate_metrics(
beta_hat = cv_fit$fit_spxlvb$beta,
posterior_inclusion_probability = cv_fit$fit_spxlvb$omega, # varies per method
sim_data = sim_data,
alpha = 0.1
)
metrics_opt_fit  <- calculate_metrics(
beta_hat = opt_fit$fit_spxlvb$beta,
posterior_inclusion_probability = opt_fit$fit_spxlvb$omega, # varies per method
sim_data = sim_data,
alpha = 0.1
)
metrics_df <- rbind(
metrics_fit,
metrics_cv_fit,
metrics_opt_fit
)
metrics_df
try(detach("package:spxlvb", unload = TRUE), silent = TRUE)
remove.packages("spxlvb")# Clear the workspace and unload the package if it's currently loaded
# Ensure all development tools are loaded
library(devtools)
library(Rcpp)
library(roxygen2)
# VERY IMPORTANT: Clean all generated files and compilation artifacts
# This specifically removes the RcppExports files and cleans up DLLs/objects
devtools::clean_dll() # This is better than manual deletion for robustness
# You can also manually check and delete if devtools::clean_dll() doesn't remove them:
file.remove("src/RcppExports.cpp")
file.remove("R/RcppExports.R")
unlink("src/*.o") # Remove object files
unlink("src/*.dll") # Remove DLLs (Windows)
# This will recreate src/RcppExports.cpp and R/RcppExports.R
writeLines('# Generated by roxygen2: do not edit by hand', "NAMESPACE")
# Clear any stuck objects
rm(list = ls())
usethis::use_build_ignore("NEWS.md")
usethis::use_build_ignore("cran-comments.md")
# usethis::use_build_ignore("WORDLIST")
# If you decide to keep github-update.txt, ignore it too:
usethis::use_build_ignore("github-update.txt")
# Run directly on the current path
Rcpp::compileAttributes(pkgdir = getwd(), verbose = TRUE)
roxygenise(clean=T)
message("Generating RcppExports and documentation...")
devtools::document(pkg = ".")
# This script builds the 'spxlvb' R package into a source archive (.tar.gz). in the parent folder of spxlvb
# Build the package into a source archive.
message("Building the package...")
devtools::build(pkg = ".")
# devtools::check(cran = TRUE)
install.packages("/home/peter/OneDrive/courses/20250729 Project 2/spxlvb_0.1.0.tar.gz", repos = NULL, type = "source")
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
library(spxlvb)
library(doParallel)
cl <- makeCluster(min(2, parallel::detectCores()))
registerDoParallel( cl)
n <- 200     # Sample size
p <- 1000    # Number of predictors
pi_0 <- 0.10 # Sparsity level (5% of predictors are active)
# Pre-calculate the Cholesky factor for a Matérn correlation matrix
L_mat <- get.L(p = p, range = 2.0, smoothness = 1.5)
# Generate the data
sim_data <- matern.data.gen(
seed_val = 42,
n = n,
p = p,
pi_0 = pi_0,
L = L_mat,
sd_beta = 4,   # Standard deviation of true signals
SNR = 2,        # Signal-to-Noise Ratio
n_test = n,
n_validation = n
)
cat("Number of active predictors:", sum(sim_data$beta != 0), "\n")
initials <- get.initials.spxlvb(
X = sim_data$X,
Y = sim_data$Y,
mu_0 = NULL,
omega_0 = NULL,
c_pi_0 = NULL,
d_pi_0 = NULL,
tau_e = NULL,
update_order = NULL,
seed = 12376
)
## Fit the Parameter Exploded Variational Bayes model
fit <- spxlvb(
X = sim_data$X,
Y = sim_data$Y,
mu_0 = initials$mu_0,
omega_0 = initials$omega_0,
c_pi_0 = initials$c_pi_0,
d_pi_0 = initials$d_pi_0,
tau_e = initials$tau_e,
update_order = initials$update_order,
mu_alpha = rep(1, ncol(sim_data$X) + 1),
alpha_prior_precision = 1000,
b_prior_precision = rep(1, ncol(sim_data$X)),
standardize = T,
intercept = T,
max_iter = 1000,
tol = 0.001,
seed = 12376
)
cat("Model converged in", fit$iterations, "iterations.\n")
# Define search grids
alpha_prior_precision_grid <- c(0, 5, 10, 100, 1000)
# Perform 5-fold cross-validation
cv_fit <- cv.spxlvb.fit(
k = 5,
X = sim_data$X,
Y = sim_data$Y,
mu_0 = initials$mu_0,
omega_0 = initials$omega_0,
c_pi_0 = initials$c_pi_0,
d_pi_0 = initials$d_pi_0,
tau_e = initials$tau_e,
update_order = initials$update_order,
mu_alpha = rep(1, ncol(sim_data$X) + 1),
alpha_prior_precision_grid = alpha_prior_precision_grid,
b_prior_precision = rep(1, ncol(sim_data$X)), # Fixed slab precision for CV
standardize = TRUE,
intercept = TRUE,
max_iter = 100L,
tol = 1e-05,
seed = 12376,
verbose = TRUE,
parallel = TRUE
)
cv_fit$CVE
# Define search grids
alpha_prior_precision_grid <- c(0, 5, 10, 100, 1000)
b_prior_precision_grid <- c(0.1, 0.5,1,1.5)
opt_fit <- grid.search.spxlvb.fit(
X = sim_data$X,
Y = sim_data$Y,
X_validation = sim_data$X_validation,
Y_validation = sim_data$Y_validation,
beta_true = NULL, # For no oracle tunning and prefiction MSE instead
mu_0 = initials$mu_0,
omega_0 = initials$omega_0,
c_pi_0 = initials$c_pi_0,
d_pi_0 = initials$d_pi_0,
tau_e = initials$tau_e,
update_order = initials$update_order,
mu_alpha = rep(1, ncol(sim_data$X) + 1),
alpha_prior_precision_grid = alpha_prior_precision_grid,
b_prior_precision_grid = b_prior_precision_grid,
standardize = TRUE,
intercept = TRUE,
max_iter = 100L,
tol = 1e-05,
seed = 12376,
verbose = TRUE,
parallel = TRUE
)
opt_fit$optimal_hyper
cat("Optimal Alpha Precision:", opt_fit$optimal_hyper[1], "\n")
plot(sim_data$beta)
plot(fit$beta)
plot(opt_fit$fit_spxlvb$beta)
plot(cv_fit$fit_spxlvb$beta)
calculate_metrics <- function(
beta_hat,
posterior_inclusion_probability,
sim_data,
alpha = 0.1
) {
# Predictions
eta_true_test <- sim_data$X_test %*% sim_data$beta
eta_hat_test <- cbind(1,sim_data$X_test) %*% beta_hat
# Estimation Error
mse_beta <- mean((sim_data$beta - beta_hat[-1])^2)
mse_eta  <- mean((eta_true_test - eta_hat_test)^2)
# Selection Performance (using local false discovery rate)
lfdr_val = 1 - posterior_inclusion_probability
lfdr_val[is.na(lfdr_val)] <- 1
threshold <- 0
if (min(lfdr_val) < alpha) {
threshold <- max(sort(lfdr_val)[cumsum(sort(lfdr_val)) < alpha])
}
# Then, you would select all predictors where
#posterior_inclusion_probability >= 1-threshold.
# Confusion matrix components
TP <- (posterior_inclusion_probability >= (1-threshold)) & (sim_data$beta != 0)
FP <- (posterior_inclusion_probability >= 1-threshold) & (sim_data$beta == 0)
FN <- (posterior_inclusion_probability < 1-threshold) & (sim_data$beta != 0)
TPR <- sum(TP) / max(sum(TP) + sum(FN), 1)
FDR <- sum(FP) / max(sum(TP) + sum(FP), 1)
return(
list(
mse_beta = mse_beta,
mse_eta = mse_eta,
TPR = TPR,
FDR = FDR)
)
}
metrics_fit <- calculate_metrics(
beta_hat = fit$beta,
posterior_inclusion_probability = fit$omega, # varies per method
sim_data = sim_data,
alpha = 0.1
)
metrics_cv_fit <- calculate_metrics(
beta_hat = cv_fit$fit_spxlvb$beta,
posterior_inclusion_probability = cv_fit$fit_spxlvb$omega, # varies per method
sim_data = sim_data,
alpha = 0.1
)
metrics_opt_fit  <- calculate_metrics(
beta_hat = opt_fit$fit_spxlvb$beta,
posterior_inclusion_probability = opt_fit$fit_spxlvb$omega, # varies per method
sim_data = sim_data,
alpha = 0.1
)
metrics_df <- rbind(
metrics_fit,
metrics_cv_fit,
metrics_opt_fit
)
metrics_df
cv_fit$CVE
# VERY IMPORTANT: Clean all generated files and compilation artifacts
# This specifically removes the RcppExports files and cleans up DLLs/objects
devtools::clean_dll() # This is better than manual deletion for robustness
# You can also manually check and delete if devtools::clean_dll() doesn't remove them:
file.remove("src/RcppExports.cpp")
file.remove("R/RcppExports.R")
unlink("src/*.o") # Remove object files
unlink("src/*.dll") # Remove DLLs (Windows)
# This will recreate src/RcppExports.cpp and R/RcppExports.R
writeLines('# Generated by roxygen2: do not edit by hand', "NAMESPACE")
# Clear any stuck objects
rm(list = ls())
usethis::use_build_ignore("NEWS.md")
usethis::use_build_ignore("cran-comments.md")
# usethis::use_build_ignore("WORDLIST")
# If you decide to keep github-update.txt, ignore it too:
usethis::use_build_ignore("github-update.txt")
# Run directly on the current path
Rcpp::compileAttributes(pkgdir = getwd(), verbose = TRUE)
roxygenise(clean=T)
message("Generating RcppExports and documentation...")
devtools::document(pkg = ".")
# This script builds the 'spxlvb' R package into a source archive (.tar.gz). in the parent folder of spxlvb
# Build the package into a source archive.
message("Building the package...")
devtools::build(pkg = ".")
devtools::check(cran = TRUE)
install.packages("/home/peter/OneDrive/courses/20250729 Project 2/spxlvb_0.1.0.tar.gz", repos = NULL, type = "source")
