% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tune_spxlvb.R
\name{tune_spxlvb}
\alias{tune_spxlvb}
\title{Tune Hyperparameters for spxlvb}
\usage{
tune_spxlvb(
  X,
  Y,
  criterion = c("elbo", "cv", "validation"),
  alpha_prior_precision_grid = c(0, 10^(3:7)),
  b_prior_precision_grid = NULL,
  b_prior_precision = rep(1, ncol(X)),
  k = 5L,
  X_validation = NULL,
  Y_validation = NULL,
  beta_true = NULL,
  mu_0 = NULL,
  omega_0 = NULL,
  c_pi_0 = NULL,
  d_pi_0 = NULL,
  tau_e = NULL,
  update_order = NULL,
  mu_alpha = NULL,
  standardize = TRUE,
  intercept = TRUE,
  max_iter = 100L,
  tol = 1e-05,
  seed = 12376,
  verbose = TRUE,
  parallel = TRUE,
  update_pi = FALSE,
  save_history = FALSE,
  selection_elbo = c("expanded_model", "original_model"),
  disable_global_alpha = FALSE
)
}
\arguments{
\item{X}{Numeric matrix (\eqn{n \times p}). Training design matrix.}

\item{Y}{Numeric vector of length \eqn{n}. Training response.}

\item{criterion}{Character string specifying the tuning strategy.
One of:
\describe{
\item{\code{"elbo"} (default)}{Fit the model on the full training
data for every grid combination and select the hyperparameters
that maximise the Evidence Lower Bound (ELBO). No data splitting
is performed. This is the recommended criterion when no external
validation set is available; see the paper Appendix for an
empirical comparison with cross-validation.}
\item{\code{"cv"}}{Perform k-fold cross-validation. For each
grid combination, the model is fit on \eqn{k-1} folds and
evaluated via mean squared prediction error (MSPE) on the
held-out fold. The combination with the lowest mean MSPE is
selected, and the final model is refit on the full training
data.}
\item{\code{"validation"}}{Fit on the training data, evaluate
MSPE on the held-out validation set (\code{X_validation},
\code{Y_validation}). The final model is refit on the
combined training + validation data.}
}}

\item{alpha_prior_precision_grid}{Numeric vector. Grid of expansion
prior precision (\eqn{\tau_\alpha}) values to search over.
Default: \code{c(0, 10^(3:7))}.}

\item{b_prior_precision_grid}{Optional numeric vector. Grid of scalar
slab prior precisions (\eqn{\tau_b}) to search over. When
\code{NULL} (default), only \code{alpha_prior_precision_grid} is
searched (1D tuning), and the fixed \code{b_prior_precision} is
used for every fit. When non-\code{NULL}, a 2D grid search over
all combinations of \eqn{(\tau_\alpha, \tau_b)} is performed. Each
scalar value is expanded to a constant vector of length \eqn{p} via
\code{rep(value, p)}.}

\item{b_prior_precision}{Numeric vector of length \eqn{p}.
Fixed slab prior precisions used when \code{b_prior_precision_grid}
is \code{NULL}. Default: \code{rep(1, ncol(X))}.}

\item{k}{Integer. Number of folds for cross-validation. Only used
when \code{criterion = "cv"}. Must be at least 3. Default: 5.}

\item{X_validation}{Optional numeric matrix. Validation design matrix.
Required when \code{criterion = "validation"}.}

\item{Y_validation}{Optional numeric vector. Validation response.
Required when \code{criterion = "validation"}.}

\item{beta_true}{Optional numeric vector of true coefficients (length
\eqn{p} or \eqn{p+1}). When supplied alongside
\code{criterion = "validation"}, the oracle linear-predictor MSE
(\eqn{\|X\beta_{true} - X\hat\beta\|^2/n}) is computed for each
grid point and reported in \code{tuning_details$grid}. Selection
still uses MSPE against \code{Y_validation}; the oracle metric is
informational only.}

\item{mu_0}{Optional numeric vector. Initial variational means.}

\item{omega_0}{Optional numeric vector. Initial variational
inclusion probabilities.}

\item{c_pi_0}{Optional numeric. Prior Beta shape1 for \eqn{\pi}.}

\item{d_pi_0}{Optional numeric. Prior Beta shape2 for \eqn{\pi}.}

\item{tau_e}{Optional numeric. Initial error precision.}

\item{update_order}{Optional integer vector. Coordinate update
order (0-indexed for C++).}

\item{mu_alpha}{Numeric vector of length \eqn{p+1}. Prior means for
the expansion parameters (see \code{\link{spxlvb}}). Default:
a vector of ones.}

\item{standardize}{Logical. Center Y and center + scale X.
Default: \code{TRUE}.}

\item{intercept}{Logical. Include an intercept (requires
\code{standardize = TRUE}). Default: \code{TRUE}.}

\item{max_iter}{Integer. Maximum VB iterations per fit.
Default: 100.}

\item{tol}{Numeric. Convergence tolerance. Default: 1e-5.}

\item{seed}{Integer. Seed for reproducibility. Default: 12376.}

\item{verbose}{Logical. Print progress messages. Default: \code{TRUE}.}

\item{parallel}{Logical. Run grid evaluation in parallel via
\code{foreach}. A parallel backend (e.g.,
\code{doParallel::registerDoParallel()}) must be registered.
Default: \code{TRUE}.}

\item{update_pi}{Logical. If \code{TRUE}, treat \eqn{\pi} as a variational
parameter (see \code{\link{spxlvb}}). Default: \code{FALSE}.}

\item{save_history}{Logical. Store per-iteration parameter histories
in the final fit. Default: \code{FALSE}.}

\item{selection_elbo}{Character string controlling which ELBO is used
for grid selection when \code{criterion = "elbo"}. One of:
\describe{
\item{\code{"expanded_model"} (default)}{The ELBO of the expanded
model, including all expansion parameter (\eqn{\alpha}) terms:
prior normalisation, entropy, and variance penalty.}
\item{\code{"original_model"}}{The ELBO of the original
(non-expanded) model, with all \eqn{\alpha}-related terms
removed. This ELBO depends only on the data fit, spike-and-slab
prior, and the variational entropy of \eqn{(b, s)}.}
}
Ignored when \code{criterion} is \code{"cv"} or
\code{"validation"} (which select by MSPE, not ELBO).}

\item{disable_global_alpha}{Logical. If \code{TRUE}, skip the global
\eqn{\alpha_{p+1}} rescaling step (see \code{\link{spxlvb}}).
Default: \code{FALSE}.}
}
\value{
A list with elements:
\describe{
\item{\code{fit}}{The final \code{spxlvb} model object (as
returned by \code{\link{spxlvb}}).}
\item{\code{criterion}}{Character string: the criterion used
(\code{"elbo"}, \code{"cv"}, or \code{"validation"}).}
\item{\code{optimal}}{Named list with elements
\code{alpha_prior_precision} and \code{b_prior_precision}
(the selected scalar values).}
\item{\code{tuning_grid}}{Data frame with one row per grid
combination and columns \code{alpha_prior_precision},
\code{b_prior_precision}, and the tuning score(s) (ELBO
for \code{"elbo"}; mean MSPE for \code{"cv"} and
\code{"validation"}).}
\item{\code{tuning_details}}{A list of criterion-specific
diagnostics:
\itemize{
\item For \code{"cv"}: \code{per_fold_mspe} (array of
per-fold prediction errors), \code{k} (number of folds).
\item For \code{"validation"}: \code{oracle_mspe} (if
\code{beta_true} was supplied).
\item For \code{"elbo"}: empty list.
}}
\item{\code{refitted_on}}{Character string:
\code{"training"} for \code{"elbo"} and \code{"cv"};
\code{"training_plus_validation"} for \code{"validation"}.}
}
}
\description{
Selects optimal hyperparameters for the \code{\link{spxlvb}} model and
returns the final fitted model. Three tuning criteria are supported:
ELBO maximisation, k-fold cross-validation, and held-out validation.
}
\details{
\strong{How tuning works.}

The function searches over a grid of hyperparameter values. When only
\code{alpha_prior_precision_grid} is specified (and
\code{b_prior_precision_grid = NULL}), a 1D search is performed and
the fixed \code{b_prior_precision} vector is used throughout. When
both grids are specified, all combinations are evaluated (2D search).

For \code{criterion = "elbo"}, the model is fit once per grid point
on the full training data, and the fit with the highest ELBO is
returned directly (no redundant refit). This makes ELBO tuning
faster than CV, since the winning fit is already on the full data.

For \code{criterion = "cv"}, data are split into \code{k} folds
using \code{caret::createFolds}. Each grid point is evaluated on all
folds, the mean MSPE is computed, and the optimal hyperparameters are
used for a final refit on the full training data.

For \code{criterion = "validation"}, the model is fit on the training
data for each grid point, evaluated on the validation set, and the
final model is refit on the combined (training + validation) data.

\strong{Initialisation.}

Variational parameters are initialised via LASSO (see
\code{\link{get_initials_spxlvb}}). For \code{criterion = "cv"},
initialisations are computed once on the full training data and
shared across all folds and grid points, avoiding redundant
\code{cv.glmnet} calls.

\strong{Parallelism.}

When \code{parallel = TRUE}, grid points (or fold x grid-point
combinations for CV) are distributed via \code{foreach}. Register a
backend before calling:
\code{doParallel::registerDoParallel(cores = 4)}.
}
\examples{
\donttest{
set.seed(1)
n <- 50; p <- 20
X <- matrix(rnorm(n * p), n, p)
Y <- X[, 1:3] \%*\% c(1, -1, 0.5) + rnorm(n)

# ELBO tuning (default, recommended)
result <- tune_spxlvb(X, Y,
  criterion = "elbo",
  alpha_prior_precision_grid = c(100, 1000),
  b_prior_precision_grid = c(1, 5),
  parallel = FALSE)
result$optimal
result$criterion

# Cross-validation tuning
result_cv <- tune_spxlvb(X, Y,
  criterion = "cv", k = 3,
  alpha_prior_precision_grid = c(100, 1000),
  parallel = FALSE)
result_cv$optimal
}

}
\seealso{
\code{\link{spxlvb}} for the low-level fitter with fixed
hyperparameters.
}
